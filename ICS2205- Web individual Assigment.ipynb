{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4f5047",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div align=\"center\">\n",
    "<font size=\"6\">ICS2205- Web individual Assigment</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e9003",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>Required Libraries</b></font> <br>\n",
    "The nltk library is used get the list of the stopwords and for word stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94273795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5a4ca",
   "metadata": {},
   "source": [
    "If not downloaded, download the following nltk packages to be able to compile the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da73cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040365f5",
   "metadata": {},
   "source": [
    "The following are the used packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c0b43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET #To read the documents\n",
    "#NLTK\n",
    "import nltk \n",
    "from nltk.corpus import stopwords #List of stopwords\n",
    "from nltk.stem import PorterStemmer #Stemming of words\n",
    "#Other Libraries\n",
    "import re #For Tokenization\n",
    "import pandas as pd #Pandna Dataframe\n",
    "import numpy as np #Useful Vectors Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe5dde",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>Parsing The Documents</b></font> <br><br>\n",
    "All the documents are parsed and the text contents in the \"raw\" tag are extracted into a single list. The documents are stored locally to this notebook in a folder called docs. The filename is mathemitically generated, and the text data in each file is extracted.\n",
    "\n",
    "<b>Case Folding:</b> Case folding is present in this function to not have to loop through the text again to apply Case Folding. The text in converted to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a06ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Parse the document\n",
    "totalDocLength = 332\n",
    "\n",
    "def ExtractData():\n",
    "    textData = [] \n",
    "    for i in range(1,totalDocLength):\n",
    "        #Generate the string name mathematically\n",
    "        filename = \"docs\\wes2015.d\" + str(i//100) + str((i//10)%10) + str(i%10) + \".naf\"\n",
    "        doc = ET.parse(filename) # Get each document seperately\n",
    "        root = doc.getroot() # Get root\n",
    "        for child in root:\n",
    "            if child.tag == \"raw\": #If the child has the tag raw\n",
    "                textData.append(child.text.lower()) #Append the data and the case folding\n",
    "    return textData #A list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61701b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "textData = ExtractData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bc9a7",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>Tokenize The Document's Content</b></font> <br><br>\n",
    "A function was defined to tokenize the documents content. The tokenization happens at all empty spaces and punctuation. The re package was used for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f118617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizations\n",
    "def TokenizeData(txtData):\n",
    "    tknData = []\n",
    "    for i in range(0,len(txtData)):\n",
    "        tokens = re.findall(r\"[\\w']+\", txtData[i]) #Will tokenize the data by spaces and punct\n",
    "        tknData.append(tokens)\n",
    "    return tknData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e50912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['william', 'beaumont', 'and', 'the', 'human', 'digestion', 'william', 'beaumont', 'physiology', 'of', 'digestion', 'image', 'source', 'on', 'november', '21', '1785', 'us', 'american', 'surgeon', 'william', 'beaumont', 'was', 'born', 'he', 'became', 'best', 'known', 'as', 'father', 'of', 'gastric', 'physiology', 'following', 'his', 'research', 'on', 'human', 'digestion', 'william', 'beaumont', 'was', 'born', 'in', 'lebanon', 'connecticut', 'and', 'became', 'a', 'physician', 'he', 'served', 'as', 'a', 'surgeon', 's', 'mate', 'in', 'the', 'army', 'during', 'the', 'war', 'of', '1812', 'he', 'opened', 'a', 'private', 'practice', 'in', 'plattsburgh', 'new', 'york', 'but', 'rejoined', 'the', 'army', 'as', 'a', 'surgeon', 'in', '1819', 'beaumont', 'was', 'stationed', 'at', 'fort', 'mackinac', 'on', 'mackinac', 'island', 'in', 'michigan', 'in', 'the', 'early', '1820s', 'when', 'it', 'existed', 'to', 'protect', 'the', 'interests', 'of', 'the', 'american', 'fur', 'company', 'the', 'fort', 'became', 'the', 'refuge', 'for', 'a', 'wounded', '19', 'year', 'old', 'french', 'canadian', 'fur', 'trader', 'named', 'alexis', 'st', 'martin', 'when', 'a', 'shotgun', 'went', 'off', 'by', 'accident', 'in', 'the', 'american', 'fur', 'company', 'store', 'at', 'close', 'range', 'june', '6th', '1822', 'st', 'martin', 's', 'wound', 'was', 'quite', 'serious', 'because', 'his', 'stomach', 'was', 'perforated', 'and', 'several', 'ribs', 'were', 'broken', 'nobody', 'really', 'expected', 'that', 'the', 'young', 'man', 'would', 'survive', 'but', 'he', 'really', 'did', 'the', 'skin', 'around', 'st', 'martin', 's', 'wound', 'fused', 'to', 'the', 'hole', 'in', 'his', 'stomach', 'leaving', 'a', 'permanent', 'opening', 'a', 'gastric', 'fistula', '1', 'beaumont', 'quickly', 'noticed', 'that', 'there', 'was', 'much', 'research', 'potential', 'back', 'then', 'not', 'too', 'much', 'was', 'known', 'about', 'the', 'digestive', 'system', 'in', 'order', 'to', 'gain', 'more', 'information', 'beaumont', 'performed', 'numerous', 'experiments', 'on', 'st', 'martin', 'over', 'a', 'period', 'of', 'eight', 'years', 'the', 'experiments', 'must', 'have', 'been', 'really', 'uncomfortable', 'for', 'the', 'man', 'who', 'was', 'inserted', 'bits', 'of', 'different', 'foods', 'tied', 'to', 'strings', 'through', 'the', 'hole', 'in', 'his', 'stomach', 'pulling', 'them', 'out', 'periodically', 'to', 'observe', 'digestion', 'beaumont', 'also', 'removed', 'gastric', 'juice', 'examining', 'it', 'to', 'better', 'understand', 'its', 'nature', 'beaumont', 'became', 'the', 'father', 'of', 'gastric', 'physiology', 'and', 'his', 'findings', 'were', 'published', 'in', 'the', 'book', 'experiments', 'and', 'observations', 'on', 'the', 'gastric', 'juice', 'and', 'the', 'physiology', 'of', 'digestion', 'in', '1833', 'the', 'work', 'is', 'now', 'considered', 'as', 'the', 'basis', 'of', 'much', 'of', 'the', 'early', 'knowledge', 'on', 'digestion', 'william', 'beaumont', 'discovered', 'that', 'hydrochloric', 'acid', 'is', 'the', 'main', 'chemical', 'responsible', 'for', 'breaking', 'down', 'food', 'and', 'he', 'suggested', 'that', 'another', 'important', 'digestive', 'chemical', 'which', 'is', 'now', 'known', 'as', 'pepsin', 'he', 'suggested', 'that', 'digestion', 'is', 'a', 'chemical', 'process', 'not', 'merely', 'a', 'mechanical', 'one', 'caused', 'by', 'stomach', 'muscle', 'movement', 'also', 'beaumont', 'gave', 'insights', 'on', 'how', 'emotions', 'temperature', 'and', 'physical', 'activity', 'can', 'affect', 'digestion', 'beaumont', 's', 'famous', 'patient', 'st', 'martin', 'outlived', 'the', 'scientist', 'even', 'though', 'his', 'wound', 'never', 'completely', 'healed', 'he', 'had', 'several', 'children', 'and', 'died', 'at', 'the', 'age', 'of', '83', '2', 'at', 'yovisto', 'you', 'may', 'be', 'interested', 'in', 'a', 'video', 'lecture', 'on', 'the', 'digestive', 'system']\n"
     ]
    }
   ],
   "source": [
    "textData = TokenizeData(textData)\n",
    "print(textData[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d23571f",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>Stop-word removal and stemming</b></font> <br><br>\n",
    "\n",
    "Using the NLTK library the a set containing the stop words of the english langauge was initialised. If a word in the document is part of the stop word's set it is not included in the fltData list. Any words not in the stop words lists are stemmed using the NLTK PorterStemmer. The stemmed words are then appended, and at the end of the function the filitered documents content are returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d0458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWordsAndStem(txtData):\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    fltData = []\n",
    "    for doc in txtData:\n",
    "        filtered_sentence = []\n",
    "        for w in doc:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(ps.stem(w))\n",
    "        fltData.append(filtered_sentence)\n",
    "    return fltData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3a22d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "textData = RemoveStopWordsAndStem(textData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fad1c6",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>Term Frequency</b></font> <br><br>\n",
    "\n",
    "To get the term frequency the function to get a set of the unique words was implemented. The getUniqueWords() function loops through every word, in every document, in the extracted data. If a word in a document is not part of the lists it appends it the uniqueWords set. The final set of unique words is returned at the end of the function.\n",
    "\n",
    "https://www.opinosis-analytics.com/knowledge-base/term-frequency-explained/#:~:text=TF(t)%20%3D%20(Number,of%20terms%20in%20the%20document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98bc729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUniqueWords(txtData):\n",
    "    uniqueWords = set()\n",
    "    for doc in txtData:\n",
    "        for word in doc:\n",
    "            if word not in uniqueWords:\n",
    "                uniqueWords.add(word)\n",
    "    return uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fade15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = GetUniqueWords(textData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a316630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TermFrequency(doc,word):\n",
    "    docLength = len(doc)\n",
    "    \n",
    "    occurance = 0\n",
    "    for w in doc:\n",
    "        if w == word:\n",
    "            occurance += 1\n",
    "\n",
    "    return occurance / docLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4f9e2b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019157088122605363"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TermFrequency(textData[0],\"william\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8d7f6",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>Inverse Document Frequency</b></font> <br><br>\n",
    "\n",
    "Function 1\n",
    "Function 2\n",
    "Function 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f18908e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalDocuments(txtData):\n",
    "    return len(txtData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17348ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docLength = getTotalDocuments(textData)\n",
    "docLength = len(textData)\n",
    "def count_dict(txtData):\n",
    "    count_dict = {}\n",
    "    for word in uniqueWords:\n",
    "        count_dict[word] = 0\n",
    "    for doc in txtData:\n",
    "        usedWords = set()\n",
    "        for word in doc:\n",
    "            if word not in usedWords:\n",
    "                usedWords.add(word)\n",
    "                count_dict[word] += 1\n",
    "    return count_dict\n",
    "dic = count_dict(textData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18d054f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(word):\n",
    "    try:\n",
    "        word_occurance = dic[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(docLength / word_occurance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e975f0",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>TF.IDF</b></font> <br><br>\n",
    "\n",
    "To get the term frequency the function to get a set of the unique words was implemented. The getUniqueWords() function loops through every word, in every document, in the extracted data. If a word in a document is not part of the lists it appends it the uniqueWords set. The final set of unique words is returned at the end of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac749f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(doc):\n",
    "    vec = np.zeros((len(uniqueWords),))\n",
    "    for word in doc:\n",
    "        tf = TermFrequency(doc, word)\n",
    "        idf = inverse_document_frequency(word)\n",
    "        vec[uniqueWords_index[word]] = tf * idf\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5a5d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords_index = {}\n",
    "for i, word in enumerate(uniqueWords):\n",
    "    uniqueWords_index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "520840cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03582936288606776\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for doc in textData:\n",
    "    vectors.append(tf_idf(doc))\n",
    "vectorsCpy = vectors.copy()\n",
    "print(vectorsCpy[0][uniqueWords_index[\"william\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ae805",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>Displaying the TF.IDF</b></font> <br><br>\n",
    "\n",
    "Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48b84e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "      <th>doc4</th>\n",
       "      <th>doc5</th>\n",
       "      <th>doc6</th>\n",
       "      <th>doc7</th>\n",
       "      <th>doc8</th>\n",
       "      <th>doc9</th>\n",
       "      <th>doc10</th>\n",
       "      <th>...</th>\n",
       "      <th>doc322</th>\n",
       "      <th>doc323</th>\n",
       "      <th>doc324</th>\n",
       "      <th>doc325</th>\n",
       "      <th>doc326</th>\n",
       "      <th>doc327</th>\n",
       "      <th>doc328</th>\n",
       "      <th>doc329</th>\n",
       "      <th>doc330</th>\n",
       "      <th>doc331</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>comput</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014777</td>\n",
       "      <td>0.010876</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mathemat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theori</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engin</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011773</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.015068</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benzeliu</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infinito</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ennobl</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conspiraci</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strindberg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13553 rows × 331 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            doc1      doc2      doc3  doc4      doc5      doc6      doc7  \\\n",
       "comput       0.0  0.007269  0.000000   0.0  0.000000  0.000000  0.000000   \n",
       "de           0.0  0.000000  0.076090   0.0  0.000000  0.000000  0.012947   \n",
       "mathemat     0.0  0.000000  0.000000   0.0  0.000000  0.025734  0.000000   \n",
       "theori       0.0  0.000000  0.000000   0.0  0.010375  0.003411  0.005994   \n",
       "engin        0.0  0.000000  0.002581   0.0  0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...   ...       ...       ...       ...   \n",
       "benzeliu     0.0  0.000000  0.000000   0.0  0.000000  0.000000  0.000000   \n",
       "infinito     0.0  0.000000  0.000000   0.0  0.000000  0.000000  0.000000   \n",
       "ennobl       0.0  0.000000  0.000000   0.0  0.000000  0.000000  0.000000   \n",
       "conspiraci   0.0  0.000000  0.000000   0.0  0.000000  0.000000  0.000000   \n",
       "strindberg   0.0  0.000000  0.000000   0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "            doc8  doc9  doc10  ...  doc322    doc323    doc324    doc325  \\\n",
       "comput       0.0   0.0    0.0  ...     0.0  0.000000  0.000000  0.000000   \n",
       "de           0.0   0.0    0.0  ...     0.0  0.000000  0.014777  0.010876   \n",
       "mathemat     0.0   0.0    0.0  ...     0.0  0.000000  0.000000  0.000000   \n",
       "theori       0.0   0.0    0.0  ...     0.0  0.002402  0.000000  0.003357   \n",
       "engin        0.0   0.0    0.0  ...     0.0  0.002640  0.000000  0.000000   \n",
       "...          ...   ...    ...  ...     ...       ...       ...       ...   \n",
       "benzeliu     0.0   0.0    0.0  ...     0.0  0.000000  0.000000  0.000000   \n",
       "infinito     0.0   0.0    0.0  ...     0.0  0.000000  0.000000  0.000000   \n",
       "ennobl       0.0   0.0    0.0  ...     0.0  0.000000  0.000000  0.000000   \n",
       "conspiraci   0.0   0.0    0.0  ...     0.0  0.000000  0.000000  0.000000   \n",
       "strindberg   0.0   0.0    0.0  ...     0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "              doc326  doc327    doc328    doc329    doc330    doc331  \n",
       "comput      0.000000     0.0  0.000000  0.000000  0.000000  0.000000  \n",
       "de          0.010789     0.0  0.037602  0.000000  0.000000  0.064975  \n",
       "mathemat    0.000000     0.0  0.002526  0.004548  0.002586  0.000000  \n",
       "theori      0.000000     0.0  0.000000  0.000000  0.000000  0.000000  \n",
       "engin       0.000000     0.0  0.011773  0.021200  0.015068  0.000000  \n",
       "...              ...     ...       ...       ...       ...       ...  \n",
       "benzeliu    0.000000     0.0  0.000000  0.000000  0.000000  0.000000  \n",
       "infinito    0.000000     0.0  0.000000  0.000000  0.000000  0.000000  \n",
       "ennobl      0.000000     0.0  0.000000  0.000000  0.000000  0.000000  \n",
       "conspiraci  0.000000     0.0  0.000000  0.000000  0.000000  0.000000  \n",
       "strindberg  0.000000     0.0  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[13553 rows x 331 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_idf2 = pd.DataFrame(data = vectors)\n",
    "tf_idf2.columns = uniqueWords_index\n",
    "tfidf_matrix = tf_idf2.T\n",
    "tfidf_matrix.columns = ['doc'+ str(i) for i in range(1, docLength+1)]\n",
    "tfidf_matrix['count'] = tfidf_matrix.sum(axis=1)\n",
    "tfidf_matrix = tfidf_matrix.sort_values(by ='count', ascending=False)\n",
    "display(tfidf_matrix.drop(columns=['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108804d",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>Query</b></font> <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ee881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"William beaumont\"\n",
    "#Case Folding\n",
    "query = query.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3cb3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "252a1a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['william', 'beaumont']\n"
     ]
    }
   ],
   "source": [
    "def tokenizeQuery(query):\n",
    "    tokens = re.findall(r\"[\\w']+\", query)\n",
    "    return tokens\n",
    "\n",
    "def RemoveStopWordsAndStemQuery(query):\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = []\n",
    "    for w in query:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(ps.stem(w))\n",
    "    return filtered_sentence\n",
    "    \n",
    "query = tokenizeQuery(query)\n",
    "query = RemoveStopWordsAndStemQuery(query)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75fa9d",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>Cosine similiarity</b></font> <br><br>\n",
    "\n",
    "https://www.geeksforgeeks.org/python-measure-similarity-between-two-sentences-using-cosine-similarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d0da51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineVal(n):  \n",
    "    return n[1]\n",
    "\n",
    "def sort(simTuple):\n",
    "    return sorted(simTuple, key = cosineVal, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83d68534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity\n",
    "Y_set = {w for w in query} #Grab Query\n",
    "sim = []\n",
    "\n",
    "for i in range (0,len(textData)):\n",
    "    c = 0\n",
    "    l1 =[]\n",
    "    l2 =[]\n",
    "    X_set = {}\n",
    "    rvector = []\n",
    "    # remove stop words from the string\n",
    "    X_set = {w for w in textData[i]} #Grab Document 0\n",
    "    \n",
    "    rvector = X_set.union(Y_set) #Union Them\n",
    "    #print(rvector)\n",
    "    \n",
    "    \n",
    "    for w in rvector:\n",
    "        if w in X_set: \n",
    "            l1.append(vectorsCpy[i][uniqueWords_index[w]]) #Value of Wieghting for that document\n",
    "        else: \n",
    "            l1.append(0)\n",
    "            \n",
    "        if w in Y_set:\n",
    "            l2.append(1)\n",
    "        else: \n",
    "            l2.append(0)\n",
    "\n",
    "    # cosine formula \n",
    "    for j in range(len(rvector)):\n",
    "            c+= l1[j]*l2[j]\n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "    if(cosine != 0):\n",
    "        sim.append(((\"wes2015.d\"+str(i+1)),cosine))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc52ea",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>Ranking</b></font> <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8aaccdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Document: wes2015.d1  \t --- Cosine Similarity: 0.1192\n",
      "2. Document: wes2015.d273  \t --- Cosine Similarity: 0.0241\n",
      "3. Document: wes2015.d310  \t --- Cosine Similarity: 0.0162\n",
      "4. Document: wes2015.d69  \t --- Cosine Similarity: 0.0134\n",
      "5. Document: wes2015.d28  \t --- Cosine Similarity: 0.0113\n",
      "6. Document: wes2015.d320  \t --- Cosine Similarity: 0.0112\n",
      "7. Document: wes2015.d136  \t --- Cosine Similarity: 0.0108\n",
      "8. Document: wes2015.d330  \t --- Cosine Similarity: 0.0107\n",
      "9. Document: wes2015.d102  \t --- Cosine Similarity: 0.01\n",
      "10. Document: wes2015.d78  \t --- Cosine Similarity: 0.0091\n"
     ]
    }
   ],
   "source": [
    "sim = sort(sim)\n",
    "counter = 1\n",
    "for i in sim[:10]:\n",
    "    print(str(counter)+\".\", \"Document:\", i[0],\" \\t --- Cosine Similarity:\", round(i[1],4))\n",
    "    counter+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
